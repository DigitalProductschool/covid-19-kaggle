{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()\n\nmeta_df.info()\n\nall_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(all_json)\n\n############\n#Helper function\n############\n\n#file reader class\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            try:\n                self.paper_id = content['paper_id']\n            except Exception as e:\n                self.paper_id = ''\n            self.abstract = []\n            self.body_text= []\n            \n            # Abstract\n            try:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            except Exception as e:\n                pass\n            # Body text\n            \n            try:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            except Exception as e:\n                pass\n            \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}:{self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)\n\n\n#Helper function adds break after every words when character\n# length reach to certain amount. This is for the interactive plot so \n#that hover tool fits the screen.\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\n\n\n########################\n    ##Load the Data into DataFrame\n######################\n\n#Using the helper functions, let's read in the articles into a \n#DataFrame that can be used easily:\n    \ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 100) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id','abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = None\n\ndf_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid.head()\n\n\n\n\ndf_covid.info()\n\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n\ndf_covid['abstract'].describe(include='all')\ndf_covid['body_text'].describe(include='all')\ndf_covid.info()\n\n#It looks like we didn't have duplicates. Instead, it was articles without Abstracts.\n\n\n# drop Null vales:\ndf_covid.dropna(inplace=True)\ndf_covid.info()\n\n\n\n#removing punctuation from each text\nimport re\n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n\n#convert to lower text\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))\n\n#labelling language\nfrom langdetect import detect\ndf_covid['langue'] = df_covid['title'].apply(detect)\n\n# df_covid.to_csv(r'/kaggle/input/CORD-19-research-challenge/df_covid_lang_labels.csv', index=False)\n\n\n#keeping only english language:\ndf_covid = df_covid.loc[df_covid['langue'] == 'en']\n# df_covid.to_csv(r'/kaggle/input/CORD-19-research-challenge/df_covid_en_only.csv', index=False)\n\n\ntext = df_covid.drop([\"paper_id\", \"abstract\", \"abstract_word_count\", \"body_word_count\", \"authors\", \"title\", \"journal\", \"abstract_summary\", \"langue\"], axis=1)\ntext_arr = text.stack().tolist()\n\n\n\ndf_covid.info()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom gensim.models import Doc2Vec\nimport gensim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LabeledSentence1 = gensim.models.doc2vec.TaggedDocument\nall_content_train = []\nj=0\nfor em in df_covid['body_text'].values:    \n    all_content_train.append(LabeledSentence1(em,[j]))\n    j+=1\n    print(\"Number of texts processed: \", j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_covid['body_text'].values)\nX2 = vectorizer.transform(df_covid['body_text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nclf = TruncatedSVD(100)\nXpca = clf.fit_transform(X)\n\n# pca = PCA(n_components=100).fit(X2)\n# datapoint = pca.transform(X2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2v_model = Doc2Vec(all_content_train, size = 100, window = 10, min_count = 500, workers=7, dm = 1,alpha=0.025, min_alpha=0.001)\nd2v_model.train(all_content_train, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_doc2vec = d2v_model.docvecs.doctag_syn0\nX_out = np.append(Xpca, X_doc2vec, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = kmeans_model.fit_predict(X_out)\npca = PCA(n_components=2).fit(X_out)\ndatapoint = pca.transform(X_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(labels)))\n\n# plot\nsns.scatterplot(datapoint[:, 0], datapoint[:, 1], hue=labels, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - doc2vec body text\")\nplt.savefig(\"/kaggle/working/t-sne_covid19_label_TFIDF_Doc2Vec.png\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_model = KMeans(n_clusters=9, init='k-means++', max_iter=100) \nkmeans_model.fit(X_out)\nlabels=kmeans_model.labels_.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = kmeans_model.fit_predict(Xpca)\npca = PCA(n_components=2).fit(Xpca)\ndatapoint = pca.transform(Xpca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_model = KMeans(n_clusters=9, init='k-means++', max_iter=100) \nX = kmeans_model.fit(d2v_model.docvecs.doctag_syn0)\nlabels=kmeans_model.labels_.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = kmeans_model.fit_predict(d2v_model.docvecs.doctag_syn0)\npca = PCA(n_components=2).fit(d2v_model.docvecs.doctag_syn0)\ndatapoint = pca.transform(d2v_model.docvecs.doctag_syn0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure\n# label1 = [\"#FFFF00\", \"#008000\", \"#0000FF\", “#800080”]\n# color = [label1[i] for i in labels]\nplt.scatter(datapoint[:, 0], datapoint[:, 1] )\n\ncentroids = kmeans_model.cluster_centers_\ncentroidpoint = pca.transform(centroids)\nplt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c=\"#000000\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\ndef read_corpus(df, column, tokens_only=False):\n    \"\"\"\n    Arguments\n    ---------\n        df: pd.DataFrame\n        column: str \n            text column name\n        tokens_only: bool\n            wether to add tags or not\n    \"\"\"\n    for i, line in enumerate(df[column]):\n        \n        tokens = gensim.parsing.preprocess_string(line)\n        if tokens_only:\n            yield tokens\n        else:\n            # For training data, add tags\n            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport random\nfrac_of_articles = 0.7\ntrain_df  = df_covid.sample(frac=frac_of_articles, random_state=42)\ntrain_corpus = (list(read_corpus(train_df, 'abstract'))) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# using distributed memory model\nmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=100, min_count=2, epochs=100, seed=42, workers=3, verbose=True)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_doc_vector(doc):\n    tokens = gensim.parsing.preprocess_string(doc)\n    vector = model.infer_vector(tokens)\n    return vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nabstract_vectors = model.docvecs.vectors_docs\narray_of_tasks = [get_doc_vector(task) for task in list_of_tasks]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}