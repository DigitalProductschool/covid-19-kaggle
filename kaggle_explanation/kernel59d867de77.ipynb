{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Goal of this Notebook is to solve task 3:\n**What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n**"},{"metadata":{},"cell_type":"markdown","source":"With the help of Natural language processing we are aiming to help our doctors and researchers. Our hope is to providea faster and better access to the information that has been piublished by other researchers. This access should be able to provide valuable insights from a huge database.\n\nto be able to provide valuable insights, we have interviewd  Prof. Dr. Hassan Vahidnezhad ,Faculty professor of Jefferson Institute of Molecular Medicine and the Department of Dermatology and Cutaneous Biology, Thomas Jefferson University Hospital, Philadelphia, PA.\nhe helped us to understand how we have to define our objectives to reach the goal of task 3. he has defined 4 subcategories that could fit into mentioned task.\n\n1. Characteristics\n2. Virulence\n3. Origin\n4. Role of virus genetics in Infection and treatment\n\nin order to be able to gain insights from text with computer vectorization with NLP comes into play. however it is a matter of great importance to find out which vectorization is understanding better the meaning of text and their priorization. since the database available for us is without any label, therefore we need to follow unsupervised learning to achieve our goal. one way to realize if our methodology and chosen path is correct is through visualization.\nflexibility of scatter plots for use in exploratory analysis has been proven, thus scatterplots are a very effective way of presentations and gaining valuable insights regarding data. however as data grows in scale and complexity some editions to the traditional scatter plots needs to be implemented.In order to achieve the goal of visualization it is a matter of great importance to identify the factors that influence the appropriateness of desired scatterplot. these factors should be defined based on specific data characterisitics and tasks.Thereafter valuable insights could be presented as a table or histogram in final steps.\n"},{"metadata":{},"cell_type":"markdown","source":"# ***Our approach in steps ***\n\n\n1. reading the data\n2. cleaning the data (making the data more managable, keeping the documents that are helpful to us, removing noise and outliers with respect to this specific task)\n3. preprocessing the texts effectively\n4. with the help of NLP and method .... vectorized the body text of each article\n5. reducing the dimension of each document vector to 2 with the help pf PCA method (Principal Component Analysis)\n6. with the help elbow method finding the optimal number of clustering\n7. make the optimal clustering of the similar research papers\n    (visualized in 2 dimension plot)\n8. Investigate the clusters visually on the plot\n9. calculate the accuracy and f1 score of clusters with the help of KNN method\n10. \n11. \n12. \n13. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# ***Loading the data**\n** this part of our notebook is inspired by a notebook from MaksimEkin in kaggle**\n\ncite: @inproceedings{COVID-19 Literature Clustering,\n    author = {Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward},\n    title = {COVID-19 Literature Clustering},\n    year = {2020},\n    month = {April},\n    location = {University of Maryland Baltimore County (UMBC), Baltimore, MD, USA},\n    note={Malware Research Group},\n    url = {\\url{https://github.com/MaksimEkin/COVID19-Literature-Clustering}},\n    howpublished = {TBA}\n} "},{"metadata":{},"cell_type":"markdown","source":"1. We are going to extract 'full body texts' , 'paper id', and 'abstract'  from jason files. \n2. We load the meta data too, because some of their characteristics like author, sha and journal name will be helpful for us.\n3. merging them all into a dataframe together\n\n* libraries that have been used in this part: \n* * panda\n* * numpy\n* * json"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"###Loading meta data###\n\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Fetch All of JSON File Path###\n\n\nall_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### helper function ###\n\n#file reader class\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            try:\n                self.paper_id = content['paper_id']\n            except Exception as e:\n                self.paper_id = ''\n            self.abstract = []\n            self.body_text= []\n            \n            # Abstract\n            try:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            except Exception as e:\n                pass\n            # Body text\n            \n            try:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            except Exception as e:\n                pass\n            \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}:{self.abstract[:200]}... {self.body_text[:200]}...'\n    \nfirst_row = FileReader(all_json[0])\nprint(first_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Load the Data into DataFrame##\n\n#Using the helper functions, let's read in the articles into a DataFrame that can be used easily:\n    \ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [] }\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 100) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    dict_['title'].append(meta_data['title'].values[0])\n    dict_['authors'].append(meta_data['authors'].values[0])\n\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id','abstract', 'body_text', 'authors', 'title', 'journal'])\ndf_covid.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing \nafter creating a dataframe we try to do some cleaning and feature engineering on our data\n1. removing duplicates (articles that are existing more than once get emitted)\n2. adding extra column to our data frame which tages the language of the papers\n3. keeping the articles that are in english language\n4. based on our user interview (Dr...) adding extra column to our data which labels the articles based on specifications of task 3\n5. keeping the articles that should be definitley considered for task 3 thus reducing the noise\n6. preparing the text to be ready for vectorization in the next steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"##droping the duplicates##\ndict_ = None\n\ndf_covid.info()\n\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n\ndf_covid['abstract'].describe(include='all')\ndf_covid['body_text'].describe(include='all')\n\n\n## droping the null values##\n# drop Null vales:\ndf_covid.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n!{sys.executable} -m pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## labelling the languages of articles in a separate column##\n\nfrom langdetect import detect\ndf_covid['language'] = df_covid['title'].apply(detect)\n\n#checking what languages are present in the dataset\nuniqueValues = df_covid['language'].unique()\n\ndf_covid.language.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(10,5))\nsns.set_style('darkgrid')\nsns.countplot(x='language',data=df_covid,palette='viridis')\nplt.title('language distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Taking a look at the distribution of languages in available articles in this data set, \nit is logical to keep only the papers with english language only **"},{"metadata":{"trusted":true},"cell_type":"code","source":"## keeping english language only ##\n\ndf_covid = df_covid.loc[df_covid['language'] == 'en']\ndf_covid.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*based on our user interview (Dr...) adding extra column to our data which labels the articles based on specifications of task 3*\nwe concluded that we should label the articles based on specific keywords that are related to **'Covid 19'** , **'SARS'**, **'MERS', 'Proteins'** and **'Genomes'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding papers takling about covid and sars and mers, proteins, genome\n\n\n# a function that will work as a searching machine to find specific keywords in papers\nimport re\ndef pattern_searcher(search_str:str, search_list:str):\n\n    search_obj = re.search(search_list, search_str)\n    if search_obj :\n        return True\n    else:\n        return False\n    \n###keywords related to covid that we are looking for in literatures  \ncovid_list = ['covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'novel corona',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b']\npattern_covid = '|'.join(covid_list)\n\n#adding extra column that tags papers that are related to covid19\ndf_covid['covid_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_covid))\n\n\n\n###keywords related to SARS that we are looking for in literatures\nsars_list = [r'\\bsars\\b','severe acute respiratory syndrome']\npattern_sars = '|'.join(sars_list)\n\n#adding extra column that tags papers that are related to SARS\ndf_covid['sars_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_sars))\n\n\n\n\n###keywords related to MERS that we are looking for in literatures\nmers_list = [r'\\bmers\\b','middle east respiratory syndrome']\npattern_mers = '|'.join(mers_list)\n\n#adding extra column that tags papers that are related to MERS\ndf_covid['mers_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_mers))\n\n\n\n###keywords related to protein and receptor surface protein that we are looking for in literatures\nprotein_list = ['protein','surface protein', 'proteome', 'proteomics', 'proteomic','proteins']\npattern_protein = '|'.join(protein_list)\n\n#adding extra column that tags papers that are related to Protein\ndf_covid['protein_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_protein))\n\n\n\n###keywords related to genomic sequences that we are looking for in literatures\ngenom_list=['genomic','genomics', 'genome', 'genome sequence', 'genome-wide']\npattern_genom = '|'.join(genom_list)\n\n#adding extra column that tags papers that are related to Genomic sequences\ndf_covid['genom_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_genom))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### only keeping the papers that should be most related to task3 ###\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Only keeping the papers that should be the most related to task 3**\ntherefore we decided to keep the papers that have one of these conditions:\n1. talking abour *covid* **and** *protein* **and** *genomes*\n2. talking abour *MERS* **and** *protein* **and** *genomes*\n3. talking abour *SARS* **and** *protein* **and** *genomes*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### only keeping the papers that should be most related to task3 ###\ndf_covid_task3 = df_covid.loc[((df_covid['covid_match'] == True) & (df_covid['protein_match']== True) & (df_covid['genom_match']== True)) | ((df_covid['mers_match']== True) & (df_covid['protein_match']== True) & (df_covid['genom_match']== True)) | ((df_covid['sars_match']== True) & (df_covid['protein_match']== True) & (df_covid['genom_match']== True))]\ndf_covid_task3.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid_task3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**preparing the text to be ready for vectorization in the next steps**\n\nnow we need to convert our data into something that a computer can understand.\n1. removing useless data which we call them stopwords\n2. finding the words with the same meaning with the help of wordnet database\n3. cleaning the string\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemma = WordNetLemmatizer()\n\n\nstopword_set = set(stopwords.words('english')+['a','at','s','for','was', 'we', 'were', 'what', 'when', 'which', 'while', 'with', 'within', 'without', 'would', 'seem', 'seen','several', 'should','show', 'showed', 'shown', 'shows', 'significantly', 'since', 'so', 'some', 'such','obtained', 'of', 'often', 'on', 'our', 'overall','made', 'mainly', 'make', 'may', 'mg','might', 'ml', 'mm', 'most', 'mostly', 'must', 'each', 'either', 'enough', 'especially', 'etc','had', 'has', 'have', 'having', 'here', 'how', 'however', 'upon', 'use', 'used', 'using', 'perhaps', 'pmid','can', 'could', 'did', 'do', 'does', 'done', 'due', 'during'])\n\nstop_set=['a','at','s','for','was', 'we', 'were', 'what', 'when', 'which', 'while', 'with', 'within', 'without', 'would', 'seem', 'seen','several', 'should','show', 'showed', 'shown', 'shows', 'significantly', 'since', 'so', 'some', 'such','obtained', 'of', 'often', 'on', 'our', 'overall','made', 'mainly', 'make', 'may', 'mg','might', 'ml', 'mm', 'most', 'mostly', 'must', 'each', 'either', 'enough', 'especially', 'etc','had', 'has', 'have', 'having', 'here', 'how', 'however', 'upon', 'use', 'used', 'using', 'perhaps', 'pmid','can', 'could', 'did', 'do', 'does', 'done', 'due', 'during']\n\n#definig a function that cleans the string of full body text\ndef process(string):\n    string=' '+string+' '\n    string=' '.join([word if word not in stopword_set else '' for word in string.split()])\n    string=re.sub('\\@\\w*',' ',string)\n    string=re.sub('\\.',' ',string)\n    string=re.sub(\"[,#'-\\(\\):$;\\?%]\",' ',string)\n    string=re.sub(\"\\d\",' ',string)\n    string=re.sub(r'[^\\x00-\\x7F]+',' ', string)\n    for i in stop_set:\n        string = re.sub(' ' +i+' ', ' ', string)\n    string=\" \".join(lemma.lemmatize(word) for word in string.split())\n    string=re.sub('( [\\w]{1,2} )',' ', string)\n    string=re.sub(\"\\s+\",' ',string)\n    string=string.replace('[', '')\n    string=string.replace(']', '')\n    return string.split()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}